import streamlit as st
from transformers import pipeline
import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.summarizers.lsa import LsaSummarizer as Summarizer
from sumy.nlp.tokenizers import Tokenizer
from PyPDF2 import PdfReader
from docx import Document
from io import BytesIO
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from langdetect import detect
from googletrans import Translator
from collections import Counter
import re
import textwrap
import difflib

# ---------------------- Streamlit Setup ----------------------
st.set_page_config(page_title="Universal AI Summarizer v4", page_icon="üß†", layout="wide")
st.title("üß† Universal AI Summarizer ‚Äî v4 Pro+")
st.caption("Summarize, Evaluate, and Study ‚Äî multi-language, multi-file, and all free.")

# ---------------------- Model ----------------------
@st.cache_resource
def load_model():
    return pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

summarizer = load_model()
translator = Translator()

# ---------------------- Helper Functions ----------------------
def extract_text_from_pdf(file):
    reader = PdfReader(file)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text.strip()

def extract_text_from_docx(file):
    doc = Document(file)
    return "\n".join([p.text for p in doc.paragraphs]).strip()

def web_fetch(query):
    try:
        url = f"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}"
        res = requests.get(url, timeout=5)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            return " ".join([p.text for p in soup.select("p")[:5]])
    except Exception:
        pass
    return ""

def summarize_text(text, length="medium"):
    if not text.strip():
        return "‚ö†Ô∏è No valid content found."
    if length == "short":
        max_len, min_len = 80, 30
    elif length == "long":
        max_len, min_len = 250, 100
    else:
        max_len, min_len = 150, 60
    try:
        if len(text.split()) > 800:
            parser = PlaintextParser.from_string(text, Tokenizer("english"))
            summarizer_lsa = Summarizer()
            summary_sentences = summarizer_lsa(parser.document, 6)
            return " ".join([str(s) for s in summary_sentences])
        else:
            result = summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)
            return result[0]["summary_text"]
    except Exception as e:
        return f"‚ö†Ô∏è Error generating summary: {e}"

def export_pdf(summary):
    buffer = BytesIO()
    c = canvas.Canvas(buffer, pagesize=letter)
    width, height = letter
    text_object = c.beginText(40, height - 60)
    text_object.setFont("Helvetica", 12)
    for line in textwrap.wrap(summary, 90):
        text_object.textLine(line)
    c.drawText(text_object)
    c.save()
    buffer.seek(0)
    return buffer

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_keywords(text, top_n=10):
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    common = Counter(words).most_common(top_n)
    return [w for w, _ in common]

def analyze_tone(text):
    positive = ["good", "great", "excellent", "positive", "happy", "success"]
    negative = ["bad", "sad", "poor", "negative", "failure"]
    pos_count = sum(text.lower().count(w) for w in positive)
    neg_count = sum(text.lower().count(w) for w in negative)
    if pos_count > neg_count:
        return "üôÇ Positive"
    elif neg_count > pos_count:
        return "‚òπÔ∏è Negative"
    else:
        return "üòê Neutral"

def compare_summaries(human, ai):
    seq = difflib.SequenceMatcher(None, human, ai)
    ratio = seq.ratio() * 100
    diff = difflib.ndiff(human.split(), ai.split())
    highlighted = []
    for token in diff:
        if token.startswith("-"):
            highlighted.append(f"‚ùå **{token[2:]}**")
        elif token.startswith("+"):
            highlighted.append(f"‚úÖ **{token[2:]}**")
    return ratio, " ".join(highlighted)

def generate_notes(summary):
    lines = summary.split(". ")
    qna = []
    for line in lines[:8]:
        words = line.split()
        if len(words) > 6:
            q = f"Q: What is the main point about '{' '.join(words[:4])}'?"
            a = f"A: {line.strip()}."
            qna.append((q, a))
    return qna

# ---------------------- Sidebar ----------------------
st.sidebar.header("‚öôÔ∏è Options")
summary_size = st.sidebar.radio("Summary Length", ["short", "medium", "long"], index=1)
fetch_web = st.sidebar.checkbox("üåç Add Wikipedia context", value=True)
output_format = st.sidebar.multiselect("üì§ Export Formats", ["PDF", "TXT", "DOCX"], default=["TXT"])
multi_doc = st.sidebar.checkbox("üìö Multi-document summarization", value=False)

# ---------------------- Input Section ----------------------
if multi_doc:
    uploaded_files = st.file_uploader("üìÇ Upload multiple files (PDF, DOCX, or TXT)", type=["pdf", "docx", "txt"], accept_multiple_files=True)
else:
    uploaded_files = [st.file_uploader("üìÇ Upload a single file (PDF, DOCX, or TXT)", type=["pdf", "docx", "txt"])]

text_input = st.text_area("‚úçÔ∏è Or Paste Your Content Here", height=200)
human_summary_input = st.text_area("üßë‚Äçüíª (Optional) Paste Your Human-Written Summary for Comparison", height=150)
generate = st.button("‚ú® Generate Summary and Insights")

# ---------------------- Main Logic ----------------------
if generate:
    with st.spinner("Processing... please wait ‚è≥"):
        content = ""

        # Combine multiple docs
        for file in uploaded_files:
            if file is not None:
                if file.type == "application/pdf":
                    content += extract_text_from_pdf(file) + "\n"
                elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    content += extract_text_from_docx(file) + "\n"
                elif file.type == "text/plain":
                    content += file.read().decode("utf-8") + "\n"

        if text_input:
            content += text_input

        content = clean_text(content)
        if not content:
            st.warning("‚ö†Ô∏è Please upload or paste content.")
            st.stop()

        # Detect language
        try:
            lang = detect(content)
            st.info(f"üåê Detected language: {lang.upper()}")
            if lang != "en":
                content = translator.translate(content, src=lang, dest="en").text
                st.success("‚úÖ Translated to English for summarization.")
        except Exception:
            lang = "en"

        # Fetch Wikipedia
        if fetch_web:
            web_data = web_fetch(content.split(".")[0][:15])
            if web_data:
                content += "\n\n" + web_data

        # Chunk if long
        chunks = [content[i:i+2000] for i in range(0, len(content), 2000)]
        summaries = [summarize_text(chunk, length=summary_size) for chunk in chunks]
        final_summary = " ".join(summaries)

        # Translate back if needed
        if lang != "en":
            final_summary = translator.translate(final_summary, src="en", dest=lang).text

        st.subheader("ü™Ñ AI Summary:")
        st.write(final_summary)

        # ---------------------- Evaluation ----------------------
        if human_summary_input.strip():
            st.subheader("üìä Summary Evaluation")
            ratio, diff_text = compare_summaries(human_summary_input, final_summary)
            st.write(f"**Similarity Score:** {ratio:.2f}%")
            st.markdown(diff_text)

        # ---------------------- AI Notes ----------------------
        st.subheader("üßÆ AI Study Notes (Auto Q&A)")
        notes = generate_notes(final_summary)
        for q, a in notes:
            st.markdown(f"**{q}**  \n{a}")

        # ---------------------- Insights ----------------------
        st.subheader("üîç Text Insights")
        st.write(f"**Tone:** {analyze_tone(content)}")
        st.write(f"**Top Keywords:** {', '.join(extract_keywords(content))}")

        # ---------------------- Downloads ----------------------
        if final_summary.strip():
            if "TXT" in output_format:
                txt_bytes = final_summary.encode("utf-8")
                st.download_button("‚¨áÔ∏è Download as TXT", data=txt_bytes, file_name="summary.txt")

            if "DOCX" in output_format:
                doc = Document()
                doc.add_paragraph(final_summary)
                buf = BytesIO()
                doc.save(buf)
                buf.seek(0)
                st.download_button("‚¨áÔ∏è Download as DOCX", data=buf, file_name="summary.docx")

            if "PDF" in output_format:
                pdf_buf = export_pdf(final_summary)
                st.download_button("‚¨áÔ∏è Download as PDF", data=pdf_buf, file_name="summary.pdf", mime="application/pdf")

st.markdown("---")
st.caption("üß† Built by Aditya ‚Äî Universal AI Summarizer v4 | Hugging Face + Streamlit + Free APIs")
